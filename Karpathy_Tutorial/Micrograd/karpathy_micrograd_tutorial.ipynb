{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMgAK2Dp4MGsWpwapsrh8/4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"TvLB_CGLLGuX","executionInfo":{"status":"ok","timestamp":1694747185289,"user_tz":-360,"elapsed":5,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}}},"outputs":[],"source":["import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","source":["def f(x):\n","  return 3*x**2 - 4*x +5"],"metadata":{"id":"EfeoD5GpOMxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xs = np.arange(-5,5,0.25)\n","ys = f(xs)\n","plt.plot(xs,ys)"],"metadata":{"id":"GcuQrL0LOXPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","# function -> 3*x**2 - 4*x +5\n","# derivative -> 6*x -4\n","\n","h = 0.001\n","x = 2/3\n","\n","# the defination of a derivative==>\n","# as we increase the input to a function by h, where h->0,\n","# how does the function change i.e what is it's slope\n","(f(x+h) - f(x))/h"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBUnHzhXPM5l","executionInfo":{"status":"ok","timestamp":1693629301931,"user_tz":-360,"elapsed":20,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"a2502823-f367-48dd-8487-9af7f0be2177","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0029999999995311555"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# @title\n","h = 0.00001\n","\n","a = 2.0\n","b = -3.0\n","c = 10.0\n","\n","d1 = a*b +c\n","c += h\n","d2 = a*b + c\n","print('d1', d1)\n","print('d2', d2)\n","print('slope', (d2-d1)/h)"],"metadata":{"id":"ZZ6fURpzQyEq","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title MicroGrad\n","class Value:\n","  # _children will keep track of the variables that participates in the operation,\n","  # _op will keep track of the type of operation performed\n","\n","  def __init__(self, data, _children=(), _op='', label=''): # _children ->empty tuple\n","    self.data = data\n","    self.grad = 0.0\n","    self._backward = lambda: None\n","    self._prev = set(_children)  # self._prev -> empty set\n","    self._op = _op\n","    self.label = label\n","\n","  def __repr__(self):\n","    return f\"Value(data={self.data})\"\n","  # if don't use __rapr__ then python will print out some abstract object instead of our value\n","\n","  def __add__(self,other):\n","    # so that we can also pass in int, float as other\n","    # without it, any int, float will give error since other.data can only take 'Value' object\n","    other = other if isinstance(other, Value) else Value(other)\n","    # so if input other is 'Value' object then leave it alone, else wrap the input other into a 'Value' object\n","\n","    out = Value(self.data + other.data, (self, other), '+')\n","    # def __init__(self,     data,      _children=(), _op='')\n","\n","    def _backward():\n","      self.grad +=  out.grad\n","      other.grad +=  out.grad\n","\n","    out._backward = _backward\n","    return out\n","\n","  def __mul__(self, other):\n","    other = other if isinstance(other, Value) else Value(other)\n","    out = Value(self.data * other.data, (self, other), '*')\n","\n","    def _backward():\n","      self.grad += other.data * out.grad\n","      other.grad += self.data * out.grad\n","\n","    out._backward = _backward\n","    return out\n","\n","  def __rmul__(self, other):\n","    # if __mul__ cannot do 2*a, it will check if there is __rmul__ defined\n","    # __rmul__ will swap the operand (2*a -> a*2) and then pass it back to __mul__\n","    return self * other\n","\n","  def __radd__(self, other): # other + self\n","    return self + other\n","\n","  def __truediv__(self, other):\n","    return self * (other ** -1)\n","\n","  def __neg__(self):\n","    return self * -1\n","\n","  def __sub__(self, other):\n","    return self + (-other)\n","\n","  def __pow__(self, other):\n","    assert isinstance(other, (int, float)) # only supporting int/float for now\n","    out = Value(self.data ** other, (self, ), f'**{other}')\n","\n","    def _backward():\n","      self.grad += (other * (self.data**(other-1))) * out.grad\n","      # other.grad += ((self.grad/out.data) - (other.data/self.data))* (1/math.ln(self.data)) * out.grad\n","\n","    out._backward = _backward\n","    return out\n","\n","  # def __truediv__(self, other):\n","  #   other = other if isinstance(other, Value) else Value(other)\n","  #   out = Value(self.data / other.data, (self, other), '/')\n","  #   def _backward():\n","  #     self.grad += (1/other.data) * out.grad\n","  #     other.grad += (-1/(other.data * other.data)) * out.grad\n","\n","  #   out._backward = _backward\n","  #   return out\n","\n","  # def __sub__(self, other):\n","  #   other = other if isinstance(other, Value) else Value(other)\n","  #   out = Value(self.data - other.data, (self, other), '-')\n","  #   def _backward():\n","  #     self.grad += 1.0 * out.grad\n","  #     other.grad += -1.0 * out.grad\n","\n","  #   out._backward = _backward\n","  #   return out\n","\n","  def exp(self):\n","    x = self.data\n","    t = math.exp(x)\n","    out = Value(t, (self, ), 'exp')\n","    def _backward():\n","      self.grad += out.data * out.grad\n","\n","    out._backward = _backward\n","    return out\n","\n","  def tanh(self):\n","    x = self.data\n","    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","    out = Value(t, (self, ), 'tanh')\n","\n","    def _backward():\n","      self.grad += (1- (out.data)**2) * out.grad\n","\n","    out._backward = _backward\n","    return out\n","\n","  def backward(self):\n","    topo = []\n","    visited = set()\n","    def build_topo(v):\n","      if v not in visited:\n","        visited.add(v)\n","        for child in v._prev:\n","          build_topo(child)\n","        topo.append(v)\n","\n","    build_topo(self)\n","\n","    self.grad = 1.0\n","    for node in reversed(topo):\n","      node._backward()\n"],"metadata":{"id":"1PgoAgQESY47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Graphing\n","from graphviz import Digraph\n","\n","def trace(root):\n","  # builds a set of all nodes and edges in a graph\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # for anyvalue in the graph, create a rectangular ('record') node for it\n","    dot.node(name = uid, label= \"{%s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record')\n","    if n._op:\n","      # if this value is a result of some operation, create an op node for it\n","      dot.node(name = uid + n._op, label = n._op)\n","      # and connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    #connect n1 to the op node of n2\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"],"metadata":{"cellView":"form","id":"J4F6IP3hVmqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title simple example\n","a = Value(2.0, label='a')\n","b = Value(-3.0, label='b')\n","c = Value(10.0, label='c')\n","e = a*b ; e.label = 'e'\n","d = e+c ; d.label = 'd'\n","f = Value(-2.0, label = 'f')\n","L = d/f\n","\n","\n","#a + b # by defining __add__ function, we can add the two Value objects a and b togather just by using the '+' sign\n","      # internally python looks at the function wrapped in '__ __' and uses it as the operator\n","      # so now a + b is the same as a.__add__(b)\n","#d = a*b + c # same as --> (a.__mul__(b)).__add__(c)\n","\n","\n","# print(f\"result= {L}, children= {L._prev}, operation= {L._op}\")\n","\n","L.grad = 1.0\n","d.grad = 1.0/f.data\n","f.grad = -d.data / (f.data*f.data)\n","e.grad = d.grad * 1.0 # dL/de = dL/dd * dd/de chain rule      #1/f.data\n","c.grad = d.grad * 1.0\n","a.grad = e.grad * b.data # dL/da = dL/dd * dd/de * de/da   #b.data/f.data\n","b.grad = e.grad * a.data     #a.data/f.data"],"metadata":{"cellView":"form","id":"PUCR0KxrFRGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title optimization/small steps\n","# optimization step, setting a small learning rate 0.01 and going towards (+) the gradient\n","a.data += 0.01 * a.grad\n","b.data += 0.01 * b.grad\n","c.data += 0.01 * c.grad\n","f.data += 0.01 * f.grad\n","\n","e = a * b\n","d = e + c\n","L = d / f\n","print(L.data)\n","# and from the result we see the gradient did indeed go up or go positive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"7tCpvNRVQLSO","executionInfo":{"status":"ok","timestamp":1693629302919,"user_tz":-360,"elapsed":21,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"522bc2ae-cadb-4f1c-e946-a0b40dd898da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-1.955149253731343\n"]}]},{"cell_type":"code","source":["# @title manual derivatives\n","def lol(): # manual derivatives\n","  h= 0.0001\n","\n","  a = Value(2.0, label='a')\n","  b = Value(-3.0, label='b')\n","  c = Value(10.0, label='c')\n","  e = a*b ; e.label = 'e'\n","  d = e+c ; d.label = 'd'\n","  f = Value(-2.0, label = 'f')\n","  L = d/f\n","  L1 = L.data\n","\n","  a = Value(2.0 , label='a') # nudging a by a small amount h, a+h\n","\n","  b = Value(-3.0, label='b')\n","  c = Value(10.0, label='c')\n","  e = a*b ; e.label = 'e'\n","  d = e+c ; d.label = 'd'\n","  d.data += h\n","  f = Value(-2.0, label = 'f')\n","\n","  L = d/f\n","  L2 = L.data\n","\n","  print((L2-L1)/h,L2, L1) # dL/da, how a small change in 'a' changes the derivative of L\n","\n","lol()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"O1rel2sPCdPf","executionInfo":{"status":"ok","timestamp":1693629302920,"user_tz":-360,"elapsed":21,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"6aacb64d-41d7-4869-97bc-7fcb5c2e1144"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-0.4999999999988347 -2.00005 -2.0\n"]}]},{"cell_type":"markdown","source":["# Example using a neuron"],"metadata":{"id":"51sxeWCX9qOz"}},{"cell_type":"code","source":["# @title\n","# example using a neuron\n","\n","# inputs x1,x2\n","x1 = Value(2.0, label='x1')\n","x2 = Value(1.0, label='x2')\n","# weights w1,w2\n","w1 = Value(-3.0, label='w1')\n","w2 = Value(0.0,  label='w2')\n","# bias of the neuron\n","b = Value(8.0, label='b')\n","# x1w1 + x2w2 + b\n","x1w1 = x1*w1; x1w1.label = 'x1w1'\n","x2w2 = x2*w2; x2w2.label = 'x2w2'\n","x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n","n = x1w1x2w2 + b; n.label='n'\n","\n","o = n.tanh(); o.label ='o'\n","o.backward()\n","draw_dot(o)\n"],"metadata":{"id":"LevfjNUSSl_M","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","# example using a neuron\n","\n","# inputs x1,x2\n","x1 = Value(2.0, label='x1')\n","x2 = Value(1.0, label='x2')\n","# weights w1,w2\n","w1 = Value(-3.0, label='w1')\n","w2 = Value(0.0,  label='w2')\n","# bias of the neuron\n","b = Value(8.0, label='b')\n","# x1w1 + x2w2 + b\n","x1w1 = x1*w1; x1w1.label = 'x1w1'\n","x2w2 = x2*w2; x2w2.label = 'x2w2'\n","x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n","n = x1w1x2w2 + b; n.label='n'\n","# ---------------\n","e = (2*n).exp()\n","o = (e-1)/(e+1)\n","o.label = 'o'\n","o.backward()\n","draw_dot(o)\n","#--------------"],"metadata":{"id":"jbWSWwykmgMW","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title back-propagation\n","# Automated back-propagation\n","o.grad = 1.0\n","o._backward()\n","n._backward()\n","b._backward()\n","x1w1x2w2._backward()\n","x1w1._backward()\n","x2w2._backward()\n","# Now we need to automate calling ._backward"],"metadata":{"cellView":"form","id":"0zjxiNGX7uP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title TOPOLOGICAL SORT\n","# we must only call ._backward when the entire forward pass is done, and reorder from output to input\n","# i.e all the nodes & edges of the graph have been assigned values\n","# this can be done by 'TOPOLOGICAL SORT'\n","topo = []\n","visited = set()\n","def build_topo(v):\n","  if v not in visited:\n","    visited.add(v)\n","\n","    for child in v._prev:\n","\n","      build_topo(child)\n","\n","    topo.append(v)\n","build_topo(o)\n","topo"],"metadata":{"cellView":"form","id":"f9hE7MrB-PQJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","a = Value(3.0, label='a')\n","b = a + a; b.label = 'b'\n","b.backward()\n","draw_dot(b)"],"metadata":{"cellView":"form","id":"XVSFI8AlMfHB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","a = Value(-2.0, label='a')\n","b = Value(3.0, label='b')\n","d = a*b; d.label='d'\n","e = a+b; e.label='e'\n","f = d*e; f.label='f'\n","f.backward()\n","draw_dot(f)"],"metadata":{"cellView":"form","id":"XeKrUbuDVcti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title manual back-propagation\n","# manual back-propagation\n","o.grad = 1.0\n","# o = tanh(n)\n","n.grad = 1 - o.data **2 # do/dn = 1 - o**2\n","x1w1x2w2.grad = n.grad * 1.0 # do/d(x1w1x2w2) = do/dn * dn/d(x1w1x2w2)\n","b.grad = n.grad * 1.0 # do/db = do/dn * dn/db\n","x1w1.grad = x1w1x2w2.grad * 1.0 # do/dx1w1 = do/dn * dn/d(x1w1x2w2) * d(x1w1x2w2)/dx1w1\n","x2w2.grad = x1w1x2w2.grad * 1.0 # do/dx2w2 = do/dn * dn/d(x1w1x2w2) * d(x1w1x2w2)/dx2w2\n","\n","x1.grad = x1w1.grad * w1.data\n","w1.grad = x1w1.grad * x1.data # do/dw1 = do/dn * dn/d(x1w1x2w2) * d(x1w1x2w2)/dx1w1 * dx1w1/dw1\n","x2.grad = x1w1.grad * w2.data\n","w2.grad = x2w2.grad * x2.data # do/dw2 = do/dn * dn/d(x1w1x2w2) * d(x1w1x2w2)/dx2w2 * dx2w2/dw2"],"metadata":{"cellView":"form","id":"FvbiPbUVyKvs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***USING PYTORCH***  "],"metadata":{"id":"uBbL7PXIn6It"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"IlA-SrIC0r3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","x1 = torch.Tensor([2.0]).double()                     ; x1.requires_grad = True\n","x2 = torch.Tensor([0.0]).double()                     ; x2.requires_grad = True\n","w1 = torch.Tensor([-3.0]).double()                    ; w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double()                     ; w2.requires_grad = True\n","b = torch.Tensor([6.88135735870195432]).double()      ; b.requires_grad  = True\n","n = x1*w1 + x2*w2 + b\n","o = torch.tanh(n)\n","\n","print(o.data.item())\n","o.backward()\n","\n","print(\"--------------\")\n","print('x2', x2.grad.item())\n","print('w2', w2.grad.item())\n","print('x1', x1.grad.item())\n","print('w1', w1.grad.item())\n"],"metadata":{"id":"XkPP_JLCoOmm","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.Tensor([2.0]).dtype"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Er3DsqXMptpI","executionInfo":{"status":"ok","timestamp":1693637353791,"user_tz":-360,"elapsed":444,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"a8940e90-5f55-499c-97db-d5ee4674c475"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["torch.Tensor([2.0]).double().dtype"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4O3vP4PLpyuH","executionInfo":{"status":"ok","timestamp":1693637371194,"user_tz":-360,"elapsed":4,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"b1757d40-62de-4810-d2c1-5fb831e84c69"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float64"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["o"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtJn7iRBqB7p","executionInfo":{"status":"ok","timestamp":1693637512036,"user_tz":-360,"elapsed":3,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"1b178466-c4bb-4357-c7eb-628525b2f065"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.7071], dtype=torch.float64, grad_fn=<TanhBackward0>)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["o.item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPussmIwqYVQ","executionInfo":{"status":"ok","timestamp":1693637517177,"user_tz":-360,"elapsed":7,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"a115942d-2aaa-4a76-f29b-fd97334941a3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7070985840783353"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["# **Implementing MLP with micrograd**"],"metadata":{"id":"K63nl4jEqln-"}},{"cell_type":"code","source":["np.random.uniform(-1,1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMBoQdXzrXE0","executionInfo":{"status":"ok","timestamp":1693654567454,"user_tz":-360,"elapsed":2,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"046500c5-89b6-4f75-96b8-e143117baf75"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.2418644333566824"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["class Neuron:\n","\n","  def __init__(self, nin):\n","    self.w = [Value(np.random.uniform(-1,1)) for _ in range(nin)]\n","    self.b = Value(np.random.uniform(-1,1))\n","\n","  def __call__(self, x):\n","    # x = [2.0, 3.0]\n","    # n = Neuron(2) # initialize a neuron object with 2 inputs\n","    # n(x) here python will use __call__ function\n","    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n","    out = act.tanh()\n","    return out\n","\n","  def parameters(self):\n","    return self.w + [self.b]\n","\n","class Layer:\n","\n","  def __init__(self, nin, nout):\n","    self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","  def __call__(self, x):\n","    outs = [n(x) for n in self.neurons]\n","    return outs[0] if len(outs) == 1 else outs\n","\n","  def parameters(self):\n","    # params = []\n","    # for neuron in self.neurons:\n","    #   ps = neuron.parameters()\n","    #   params.extend(ps)\n","    # return params\n","    return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","  def __init__(self, nin, nouts):\n","    sz = [nin] + nouts\n","    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","  def __call__(self, x):\n","    for layer in self.layers:\n","      x = layer(x) # here x updates so we are not inputting same x for every layer\n","    return x\n","\n","  def parameters(self):\n","    return [p for layer in self.layers for p in layer.parameters()]\n","\n"],"metadata":{"id":"XOxhQsafqlFO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#x = [2.0, 3.0, -1.0]\n","n = MLP(3, [4, 4, 1])\n","#n(x) # here python will use __call__ function"],"metadata":{"id":"ExU9f0Q1JipS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xs = [\n","    [2.0, 3.0, -1.0],\n","    [3.0, -1.0, 0.5],\n","    [0.5, 1.0, 1.0],\n","    [1.0, 1.0, -1.0],\n","]\n","ys = [1.0, -1.0, -1.0, 1.0] # desired target\n"],"metadata":{"id":"32VeZNxMBAp9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n.parameters() # weights and biases"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8qdR-f0PYcJS","executionInfo":{"status":"ok","timestamp":1693683298439,"user_tz":-360,"elapsed":434,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"d795cbcd-8b37-4edd-db9d-576329ea91b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Value(data=-0.10632380513402562),\n"," Value(data=-0.9272302798541971),\n"," Value(data=0.6330461089538579),\n"," Value(data=-0.7888907226951398),\n"," Value(data=0.04692350932016831),\n"," Value(data=-0.1106001008947679),\n"," Value(data=-0.36618031901594517),\n"," Value(data=-0.33749055002628037),\n"," Value(data=-0.9954642550530362),\n"," Value(data=0.7377405763133),\n"," Value(data=0.4236308346653055),\n"," Value(data=0.7735155768306972),\n"," Value(data=-0.7766641102502208),\n"," Value(data=0.1314689464855885),\n"," Value(data=-0.8171003322597794),\n"," Value(data=0.5546771944342328),\n"," Value(data=0.7728321784270007),\n"," Value(data=0.1506235817543522),\n"," Value(data=-0.837837418037702),\n"," Value(data=-0.048455832216313555),\n"," Value(data=0.9979793701323036),\n"," Value(data=-0.795991784796197),\n"," Value(data=-0.3924557138041571),\n"," Value(data=0.9841825496110828),\n"," Value(data=-0.007332303560053655),\n"," Value(data=-0.42058075822778207),\n"," Value(data=-0.5070202162935493),\n"," Value(data=0.5910236478410194),\n"," Value(data=-0.45449653111508925),\n"," Value(data=-0.10141796892053567),\n"," Value(data=-0.5660072026681096),\n"," Value(data=0.5597677915284578),\n"," Value(data=0.04019537481187618),\n"," Value(data=0.16234578437590064),\n"," Value(data=-0.7375421806811568),\n"," Value(data=0.5767837493039087),\n"," Value(data=-0.6783155959538987),\n"," Value(data=-0.0011893120098411814),\n"," Value(data=-0.28752843603876066),\n"," Value(data=-0.12247449553408796),\n"," Value(data=-0.7198089720087575)]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["for k in range(20):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n","\n","  # backward\n","  for p in n.parameters():\n","    p.grad = 0.0  # resets the gradients to 0 after each weights, bias update\n","                  # otherwise the previous gradients would accumulate\n","                  # new weights and biases will give new gradients when backpropping, not accumulate with old ones\n","  loss.backward()\n","\n","  # update\n","  for p in n.parameters():\n","    p.data += -0.05 * p.grad\n","\n","\n","  print(k, loss.data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GZnJotTUkp5","executionInfo":{"status":"ok","timestamp":1693683339377,"user_tz":-360,"elapsed":432,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"733bc3e6-d4ce-4fb0-b74a-d3df54195b32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.01218490014859194\n","1 0.011942034351282237\n","2 0.011708421875058007\n","3 0.011483551377520053\n","4 0.011266948139715526\n","5 0.011058170868982111\n","6 0.010856808829054734\n","7 0.010662479259188907\n","8 0.010474825049061581\n","9 0.010293512640490337\n","10 0.010118230130685852\n","11 0.009948685554910602\n","12 0.00978460532914135\n","13 0.00962573283568365\n","14 0.009471827136727312\n","15 0.00932266180259785\n","16 0.009178023842999075\n","17 0.009037712730881785\n","18 0.008901539509745541\n","19 0.00876932597620389\n"]}]},{"cell_type":"code","source":["ypred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwqC668LW4uQ","executionInfo":{"status":"ok","timestamp":1693683341781,"user_tz":-360,"elapsed":419,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"a4373354-21cc-41d2-97fe-4e4da0d7ed68"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Value(data=0.9549348612420521),\n"," Value(data=-0.9911360735543078),\n"," Value(data=-0.9389633598576318),\n"," Value(data=0.9458297257434745)]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["draw_dot(loss)"],"metadata":{"id":"2wlQpoOxHtiG"},"execution_count":null,"outputs":[]}]}