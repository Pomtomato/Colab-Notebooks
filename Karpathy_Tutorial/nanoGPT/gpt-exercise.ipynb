{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Mtby1G-vO_Ck"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","\n","#--------hyperparameters------------\n","batch_size = 64 # 4\n","block_size = 256 # 8\n","max_iters = 5000\n","eval_interval = 500\n","learning_rate = 3e-4 # 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 384 # 32\n","n_head = 6 # 4\n","#head_size = 6 # head_size = n_embd//n_head, 64<--384//6\n","n_layer = 6\n","dropout = 0.2 # every forward and backward pass 20% of these calculations are disabled\n","#---------------------------------\n","\n","\n","torch.manual_seed(1337)\n","\n","with open('/content/drive/MyDrive/input.txt', 'r', encoding='utf-8') as f:\n","  text = f.read()\n","\n","chars = sorted(list(set(''.join(text))))\n","vocab_size = len(chars)\n","s2i = {ch:i for i, ch in enumerate(chars)}\n","i2s = {i:ch for i, ch in enumerate(chars)}\n","encode = lambda s: [s2i[c] for c in s]\n","decode = lambda l: ''.join([i2s[i] for i in l])\n","\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loader\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data)-block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval() # turn off training, turn on eval\n","    for split in [\"train\", \"val\"]:\n","        losses = torch.zeros(eval_iters) # allocate space to log the losses\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X,Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train() # turn on training, turn off eval\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\"\" one head of self-attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False) # (32,16) (n_emb, head_size)\n","        self.query = nn.Linear(n_embd, head_size, bias=False) # (32,16) (n_emb, head_size)\n","        self.value = nn.Linear(n_embd, head_size, bias=False) # (32,16) (n_emb, head_size)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size))) # (T,T)(8,8)\n","        # 'tril' is not a parameter in the pytorch module. To assign it to the module we have to add it using register_buffer\n","\n","        self.dropout = nn.Dropout(dropout) # we can spread out dropout layers right before the residual block connects to the highway\n","    def forward(self, x):\n","        B,T,C = x.shape # 4,8,32\n","        k = self.key(x) # (32,16)[4,8,32] -> B,T,head_size=16\n","        q = self.query(x) # B,T,head_size=16\n","        # compute attention scores ('affinities')\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,T) @ (B,C,T) --> (B,T,T) # scaled attention\n","        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # (B,T,T)\n","\n","        wei = F.softmax(wei, dim=-1) # (B,T,T)\n","\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) #(B,T,head_size=16)\n","        out = wei @ v # (B,T,head_size=16)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"\" multiple heads of self-attention in parallel\"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd) # this is for residual connection, called after the concatanation operation\n","        self.dropout = nn.Dropout(dropout) # we can spread out dropout layers right before the residual block connects to the highway\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1) # residual block of self attention\n","        out = self.dropout(self.proj(out)) # the projection back into the residual pathway/highway\n","         # a simple Linear layer to transform output of residual blocks back into the highway\n","        return out\n","\n","class FeedForward(nn.Module):\n","    \"\"\" a simple liner layer followed by a non-linearity\"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd), # 4 * n_embd because attention paper said so and also in the next Linear layer\n","            # done to add extra computation power to the residual block; then scaling it back done when residual block add to the highway\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd), # projection layer going back into the residual highway\n","            nn.Dropout(dropout),# we can spread out dropout layers right before the residual block connects to the highway\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of head we'd like\n","        super().__init__()\n","        head_size = n_embd//n_head\n","        self.sa = MultiHeadAttention(n_head, head_size) # communication\n","        self.ffwd = FeedForward(n_embd)  # computation\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x)) # residual connections-> x=gradient highway, self.sa(x)=residual blocks of self attention\n","        x = x + self.ffwd(self.ln2(x))\n","        # layer-norm is added to 'x' \"before\" it is being fed into self-attention and feed-forward\n","        # here normaliztion is done across the C-dim(32), so the B and T act as batch-dimension of normalization\n","        return x\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        ##self.sa_head = Head(n_embd)\n","        #self.sa_head = MultiHeadAttention(4, n_embd//4) # 4 heads(communication channel of 8-dimensional self-attention)\n","        #self.ffwd = FeedForward(n_embd)\n","        # self.blocks = nn.Sequential(\n","        #     Block(n_embd, n_head=4),\n","        #     Block(n_embd, n_head=4),\n","        #     Block(n_embd, n_head=4),\n","        #     nn.LayerNorm(n_embd),\n","        # )\n","        self.blocks = nn.Sequential(*(Block(n_embd, n_head=n_head) for _ in range(n_layer)))\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","        # idx and targets are both (B,T) tensor of integers\n","        # each input token/char-int is embedded--> tok_emb\n","        tok_emb = self.token_embedding_table(idx) # (B 4, T 8, C=n_embd 32)\n","        # the position of token in that block_size of token gets embedded ->pos_emb\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C=n_embd)\n","        x = tok_emb + pos_emb # (B, T, C=n_embd) by broadcasting\n","\n","        #x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n","        #x = self.ffwd(x) # (B,T,C)\n","\n","        x = self.blocks(x) # (B,T,C)\n","\n","        logits = self.lm_head(x) # (B, T, C=vocab_size)\n","\n","        if targets == None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B,T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens; because of we added positional encodings, idx can never be > block_size.\n","            idx_cond = idx[:, -block_size:]\n","            logits, loss = self(idx_cond) # get the predictions\n","            logits = logits[:,-1,:] # focus only one the last time step (B, C)\n","            probs = F.softmax(logits, dim=1) # (B,C) apply softmax to get probabilities\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) sample from the distribution\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) append sampled index to the running sequence\n","\n","        return idx\n","\n","\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","\n","# create a pytorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch(\"train\")\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1,1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"wGoc0w7t_oEx","executionInfo":{"status":"ok","timestamp":1701712945221,"user_tz":-360,"elapsed":18746,"user":{"displayName":"Abrar Salekin Raiyan","userId":"12726755175780803222"}},"outputId":"1b05193f-faf6-4bf5-d6c8-c64d04428e51","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}]}