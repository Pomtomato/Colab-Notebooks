{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"17EI8v6hwA0VsUiBuLoGWwehRokiSbFYw","authorship_tag":"ABX9TyPaMzpuwIlc6T56C5TwAXdh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1-_i7yrHQFW","executionInfo":{"status":"ok","timestamp":1699084021877,"user_tz":-360,"elapsed":2972870,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"dd07fd13-b783-4795-db14-742cf38e1bc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.4753, val loss 4.4709\n","step 500: train loss 2.0850, val loss 2.1491\n","step 1000: train loss 1.6629, val loss 1.8235\n","step 1500: train loss 1.4918, val loss 1.6833\n","step 2000: train loss 1.3858, val loss 1.6079\n","step 2500: train loss 1.3164, val loss 1.5617\n","step 3000: train loss 1.2613, val loss 1.5311\n","step 3500: train loss 1.2163, val loss 1.5082\n","step 4000: train loss 1.1774, val loss 1.4936\n","step 4500: train loss 1.1382, val loss 1.4831\n","\n","But with prisophecal to seek think you,\n","mercy; lamentables, the king ha fifty of the groan;\n","Where good first and lewan purstiled you no more:\n","But for my soldier when shall have wear them?\n","\n","First Citizen:\n","My lord, how chair lack upon, discharity\n","Kill'd my hence willow it. Canollo, come! her, cousin,\n","And till I claim with his plic in to truth,\n","For came to make unnurstance that loog-tren's\n","Of you weighs, I will po'd with my strong.\n","You know not what, I do say them you!\n","\n","LUCIO:\n","O three you strike no\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","\n","#--------hyperparameters------------\n","batch_size = 64 # 4\n","block_size = 256 # 8\n","max_iters = 5000\n","eval_interval = 500\n","learning_rate = 3e-4 # 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 384 # 32\n","n_head = 6 # 4\n","#head_size = 6 # head_size = n_embd//n_head, 64<--384//6\n","n_layer = 6\n","dropout = 0.2 # every forward and backward pass 20% of these calculations are disabled\n","#---------------------------------\n","\n","\n","torch.manual_seed(1337)\n","\n","with open('/content/drive/MyDrive/input.txt', 'r', encoding='utf-8') as f:\n","  text = f.read()\n","\n","chars = sorted(list(set(''.join(text))))\n","vocab_size = len(chars)\n","s2i = {ch:i for i, ch in enumerate(chars)}\n","i2s = {i:ch for i, ch in enumerate(chars)}\n","encode = lambda s: [s2i[c] for c in s]\n","decode = lambda l: ''.join([i2s[i] for i in l])\n","\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loader\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data)-block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval() # turn off training, turn on eval\n","    for split in [\"train\", \"val\"]:\n","        losses = torch.zeros(eval_iters) # allocate space to log the losses\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X,Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train() # turn on training, turn off eval\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\"\" one head of self-attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False) # (32,16) (n_emb, head_size)\n","        self.query = nn.Linear(n_embd, head_size, bias=False) # (32,16) (n_emb, head_size)\n","        self.value = nn.Linear(n_embd, head_size, bias=False) # (32,16) (n_emb, head_size)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size))) # (T,T)(8,8)\n","        # 'tril' is not a parameter in the pytorch module. To assign it to the module we have to add it using register_buffer\n","\n","        self.dropout = nn.Dropout(dropout) # we can spread out dropout layers right before the residual block connects to the highway\n","    def forward(self, x):\n","        B,T,C = x.shape # 4,8,32\n","        k = self.key(x) # (32,16)[4,8,32] -> B,T,head_size=16\n","        q = self.query(x) # B,T,head_size=16\n","        # compute attention scores ('affinities')\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,T) @ (B,C,T) --> (B,T,T) # scaled attention\n","        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # (B,T,T)\n","\n","        wei = F.softmax(wei, dim=-1) # (B,T,T)\n","\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) #(B,T,head_size=16)\n","        out = wei @ v # (B,T,head_size=16)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"\" multiple heads of self-attention in parallel\"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd) # this is for residual connection, called after the concatanation operation\n","        self.dropout = nn.Dropout(dropout) # we can spread out dropout layers right before the residual block connects to the highway\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1) # residual block of self attention\n","        out = self.dropout(self.proj(out)) # the projection back into the residual pathway/highway\n","         # a simple Linear layer to transform output of residual blocks back into the highway\n","        return out\n","\n","class FeedForward(nn.Module):\n","    \"\"\" a simple liner layer followed by a non-linearity\"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd), # 4 * n_embd because attention paper said so and also in the next Linear layer\n","            # done to add extra computation power to the residual block; then scaling it back done when residual block add to the highway\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd), # projection layer going back into the residual highway\n","            nn.Dropout(dropout),# we can spread out dropout layers right before the residual block connects to the highway\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of head we'd like\n","        super().__init__()\n","        head_size = n_embd//n_head\n","        self.sa = MultiHeadAttention(n_head, head_size) # communication\n","        self.ffwd = FeedForward(n_embd)  # computation\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x)) # residual connections-> x=gradient highway, self.sa(x)=residual blocks of self attention\n","        x = x + self.ffwd(self.ln2(x))\n","        # layer-norm is added to 'x' \"before\" it is being fed into self-attention and feed-forward\n","        # here normaliztion is done across the C-dim(32), so the B and T act as batch-dimension of normalization\n","        return x\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        ##self.sa_head = Head(n_embd)\n","        #self.sa_head = MultiHeadAttention(4, n_embd//4) # 4 heads(communication channel of 8-dimensional self-attention)\n","        #self.ffwd = FeedForward(n_embd)\n","        # self.blocks = nn.Sequential(\n","        #     Block(n_embd, n_head=4),\n","        #     Block(n_embd, n_head=4),\n","        #     Block(n_embd, n_head=4),\n","        #     nn.LayerNorm(n_embd),\n","        # )\n","        self.blocks = nn.Sequential(*(Block(n_embd, n_head=n_head) for _ in range(n_layer)))\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","        # idx and targets are both (B,T) tensor of integers\n","        # each input token/char-int is embedded--> tok_emb\n","        tok_emb = self.token_embedding_table(idx) # (B 4, T 8, C=n_embd 32)\n","        # the position of token in that block_size of token gets embedded ->pos_emb\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C=n_embd)\n","        x = tok_emb + pos_emb # (B, T, C=n_embd) by broadcasting\n","\n","        #x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n","        #x = self.ffwd(x) # (B,T,C)\n","\n","        x = self.blocks(x) # (B,T,C)\n","\n","        logits = self.lm_head(x) # (B, T, C=vocab_size)\n","\n","        if targets == None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B,T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens; because of we added positional encodings, idx can never be > block_size.\n","            idx_cond = idx[:, -block_size:]\n","            logits, loss = self(idx_cond) # get the predictions\n","            logits = logits[:,-1,:] # focus only one the last time step (B, C)\n","            probs = F.softmax(logits, dim=1) # (B,C) apply softmax to get probabilities\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) sample from the distribution\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) append sampled index to the running sequence\n","\n","        return idx\n","\n","\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","\n","# create a pytorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch(\"train\")\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1,1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"]},{"cell_type":"code","source":["# generate from the model\n","context = torch.zeros((1,1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-2hqwnwUnfK","executionInfo":{"status":"ok","timestamp":1699084610874,"user_tz":-360,"elapsed":199226,"user":{"displayName":"Abrar Raiyan","userId":"03612250197745345037"}},"outputId":"e7a043e3-d47e-4147-8aef-4add118d328a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","But summers of worse;\n","My dearly sometimes to enter claim them to hequest\n","Of my encounter in mine honour,\n","And her make transpicely thy fact. An so leights\n","Says falts here me but alone? splands again.\n","\n","VIRGILICA:\n","O edds hands me with already is hand: I had got find\n","thee was your father's well with mad delic butches:\n","Hermine is a condarsmed but spitre, and want down it.\n","How say thus thy dead stable power mouths\n","tay so; thy guilty trady took to one harm. Go all, eath.\n","Helphy to good upon the mourning right out with wrongs.\n","\n","CLIFFORD:\n","I hate have and the most parliamony and there;\n","For almost, fit show us us greens like refument:\n","What never hath deserved the find throne post.\n","\n","RICHARD:\n","Hopt then, lords, some was that haste-tate table would,\n","And some high all but the rew thereof, there was combary.\n","\n","BUCKINGHAM:\n","Thraw girl?\n","\n","KING RICHARD III:\n","I dishonour so fall the fourth by Hereford?\n","\n","BENRY:\n","Therefore the weedin do perform'd his mistress heir.\n","Go thou to hell, the rascal hour voices,\n","Aboarding imprepared, from thy loath, herives' love.\n","\n","KING RICHARD II:\n","My good lords, the bloody of my fault fled traitors,\n","Hath marrife am his ploage.\n","What stofs my liege, my soul.\n","Thy father shadow ere it come to thyself.\n","\n","KING RICHARD II:\n","But to them,\n","Have long for worse than the earld wyes,\n","This losty and with a hire shall noble queen'd,\n","Richary set Day of Clarence' blood?\n","And to play for prayers. What's the name placent I'll stood?\n","The glorion to unpropares, not quoth equal I\n","And then very windurish him friends. Ah, you have\n","Here lie a coward, which would he should make him.\n","Comestme this law of George Holting Hastings;\n","FORInce greaster for the coorse that empts with swears?\n","\n","MONTAGUS:\n","The track behot is my soul.\n","\n","Provost:\n","I think you, if you are; and never made me,\n","The drainous most fool'd toward and stands words.\n","I see, command not so.\n","\n","PHOMP OSCY:\n","At she's so in less.\n","\n","DUKE OF AUMERLE:\n","This sovery wiful, lest noble morn in hand\n","The cooler, which alleasy company sending allowers;\n","But in resalm thence as the dar?\n","\n","DUKE OF AUMERLE:\n","Respair only fortune,\n","Ay ashe dead to our coussame,\n","I return from the argue to condemn hear,\n","Father twenty deny indeed of rude and comful?\n","And marrument would not say fight, doubt the heart,\n","Which writing and there as those that world sweet,\n","Revenge, but children in the king;\n","For I far my lording bearewell fear,\n","That polonars them our that show usurp, and more\n","If they will-staves from with her wallow to reason:\n","They be bear, I'll fire you fer a said;\n","There contair, Richarge make wish a liking east,\n","His frankleys still handed this gardener.\n","Upon this melt thou were my breath, and are wh these eyes\n","Comfige no modest of harm directant, am.\n","\n","GLOUCESTER:\n","Why with thy instruments stand by thy untan son\n","Than how to no every herself; and whereto we condemn'd,\n","Suspice thou meanly and the corourallowiers.\n","They should they are see to's Palenes, and me:\n","Thou art let the proclaim of To empty.\n","Return, exoting to me should\n","Which we will even was slanderd's death in the fairer exclaim'd?\n","How falls it were it a piece on my presence.\n","\n","BRUTUS:\n","Let them no that shadow's clire of?\n","Havin something this pull's body sheel's cubs:\n","My masty unhave we here;\n","In and sensect the world is one,\n","That what is Engloman's kindling piess,\n","Hath slute in the heirs of our virtuous.\n","Call hath were, I dear hence, when you oft his court?\n","\n","LUCIO:\n","He canst thou hast pleased?\n","\n","Second German:\n","Our protecious sex have deny tiempt: now, you;\n","We could your lady's sister be embassals.\n","\n","BUCKINGHAM:\n","I think that left I will not reposs it;\n","The shecous must for your grace lips.\n","\n","GLOUCESTER:\n","They read they douce, every she did doubt:\n","Then let with their nose belies; thou, tell sweet\n","Likes himself own of us well weeps,\n","To wish to scoldignity him that takest you woulds:\n","He's return, makes thus friar, and savaged!\n","To let comfort her.\n","\n","PORTEPS:\n","DUKE OF YORK:\n","And Gly thievost men you next, the enemy had on.\n","\n","PAULINA:\n","O, well! thy husband, be adone! thy hand:\n","The cheerfeit, mim-door corow his lowbring.\n","Bringing the mouth blood ded in heart,\n","Comes by step by Duke of Norfolk,\n","Whilesh have reverend the less of thy sword?\n","\n","RATCLIFFF:\n","A shepherd man, sir, he shunds that lives and offending bread?\n","His drunking with cloik'd! Hadly sook and speed,\n","To open in the chargens, supposience loving souls!\n","But cribunes stricths navish nights,\n","So hards are upposed to our seumity;\n","That was she was struck to our arm and day.\n","\n","BUSHY:\n","No, my good, may cagive, no doubt my need;\n","I did you be dulg it make, I feel not set by thee,\n","Fifting call aure dotion, and to him the are.\n","\n","Second Gentleman:\n","Your draves free's soversarce, my great heard.\n","\n","GREMIO:\n","Well both: betray mar, we will; dear levy in thine.\n","Plays me, speak! you will; one are head your king.\n","\n","Third Huntsman:\n","Gentleman moon,\n","Harry, it loves may on the same of the hard.\n","\n","STANGELY:\n","So arrs.\n","\n","KING RICHARD II:\n","While it is that? Here one the digrinht so hut\n","Is Cominius made here iso, as he will burn\n","For meed royal at life.\n","Players, be chorm'd; by return him out\n","AUting your pack ableeding exhating.\n","\n","KING RICHARD III:\n","Now, possessible, as held now follows thou hast:\n","Such forsake their one own will tongue with his coverts,\n","Or this offess at ears thou can move the hell;\n","This forthwith a begar'd; which the slaverness;\n","Lest he were in torn, and let it hap,\n","Trues so much toges to unfold this palse,\n","To seesing us smother by me the gods;\n","Dust as occupation, shall watch make more citize;\n","Say them as much for the love amber, that you such stay,\n","Thangs before their steel shrick'd upon it gotfe?\n","Uncles them, fear the brother than I seeds.\n","Hath a noble and tide so what you depart,\n","I was beseech to unflinted.\n","\n","DUKE OF YORK:\n","So I long as we reposses me bold, for was no furth.\n","\n","KING RICHARD II:\n","My Lord How, nor uncers the matter of his lew:\n","Droop, this my body him bounch: Bohemingbroken lets the field;\n","So I'ld thrice to end it you within try.\n","\n","KING RICHARD III:\n","Believe you, now, my lord'? the numbers slow, wherey, my lords,\n","Such makes; this succeed the firest be a groan,\n","Whereft being blind my blood will be among and and\n","As in acreams of your mearings slept  our hand dance;\n","Those that did it remains stande the more delige:\n","Fly eason'd this grow murder of his charefut dasces away.\n","But the chairers that you could lay full of such meet should;\n","That encormandness life and rich battle,\n","Grace a tomp, in the will wench'd me Fresh.\n","But that we would to him wide for free waste his\n","fast ourself cry supper rags for which shall not,\n","Mown to before his nature but preasent, fave?\n","Now, so touch the wounds still were in peace and\n","our report: as there are he thinkeys nature\n","May drafter long which at honourably this fresh:\n","but 'twere our assiffairs and by.\n","\n","Lieutences:\n","Then we wis enough under commonweak agreasy\n","Methinks your enemys or maids\n","In cersel of husband, and so uses it quicklys?\n","\n","AUTOLYCUS:\n","The substance a sharp--law, as thou may perove\n","A lour old only sat furnamin leavy:\n","I have it stay and give and too deep thee,\n","Ere I friar: if thou wilt upon her: thou, I dod--\n","Nurse; and the queen of his banished pit that;\n","And there villaint with him the strength that swords his sworn.\n","And, Romeo, Romeo! hast thou long seizorn that were toges!\n","\n","ROMEO:\n","\n","FRAther Ratcline; though at our dooms and arison.\n","Romeo, good day, madam not, ay, and care it\n","The mischangest their pardon and clotch with asspers. For\n","tak here oo! the twice crafts\n","Cure means order powers to deser him,\n","He has before two. I should be that upon himself\n","But well alone!\n","Heready it curbs again,\n","The dog force courable strength till hath might\n","Unto such as afterward us as winter's\n","As a things parlant hand ell talk'd off th ugarl.\n","Army, ladies 'Where is that blind smilling sin's ear,\n","That only 't impride the posse:'Faith insquired\n","The lands from the grasse puts and facts:\n","My proof lord is one ever gracious like,\n","And togethern shed pompty there; be his cleasing night.\n","Upon thriss by the cholarge to our loveure is eye;\n","Have disth set it sound for Richard ne'er look\n","From them from her committed words on more!\n","\n","KING HENRY VI:\n","How is this that Here is our fiends and tile,\n","Unless us is all the will set as judge.\n","O, that Docbank, methinks I should say thy sorrow:\n","My lead upon the deb'selips;\n","Here'erBa kindred among curses.\n","\n","KING RICHARD I:\n","Ay, I say? Knom onator, you shall not be condemn'd,\n","That young I beseech your brace say,\n","Your had hethAs we arrise-folk.\n","\n","GLOUCESTER:\n","Clarence, this yourself, lords, with alone;\n","Then Edward your Henry true service. God for your captive,\n","Which your husband.\n","\n","HASTINGS:\n","Here comes that have\n","Comfortrance we to-day, sir, down.\n","I am noble less after from him; and you\n","Have found, then sarrange me to hear Henry.\n","\n","BENVOLIO:\n","I know you were not strike better of a knight\n","It ware to have left his more more move.\n","But I am thanking command not home.\n","\n","SICINIUS:\n","We are every will give us prisoners.\n","My armight lie not that une'er rise and my fetter!\n","\n","BRUTUS:\n","Heaven for her only keeps to speak'd his feed.\n","\n","SICINIUS:\n","He's banish a soleman combit;\n","Then her look lodging is, will it were enough\n","To part us not before the gods. Wouldst you have gone.\n","Wept us for our officer, and soj!'\n","Thou urse: she was not so grievous matter;\n","Hold then? she senses take no moretred town.\n","Cleaners; could Friend not to pury ounce,\n","As he for such father for a malentacles of\n","Londone of you. Voursel o'erweak.\n","\n","SICINIUS:\n","It is a soil.\n","\n","CORIOLEOUS:\n","If, I respect.\n","\n","SICINIUS:\n","What keep the frowns upon, and in follow the\n","better of the people, of talkings honour, both the\n","peril gown with boar them no foul entimation's\n","as levier: for we were is defarend. If you\n","hope too choose. Be here find a little hearing muttinies,\n","To crossing your soul come are palty.\n","Why should I have perforce this greest of all,\n","And how we pass'd us soon my unsweed mellow.\n","\n","Provost:\n","Good son, take your child: we cannot great now two\n","Of hopes; or who in these ill-be speedying,\n","And then perforb yourselves, on therefore 'lay\n","see you, lords, were sit, as it left a favourith\n"]}]}]}